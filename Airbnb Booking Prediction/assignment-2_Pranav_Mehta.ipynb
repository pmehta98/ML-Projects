{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c448a89-afab-47d2-b8b5-9620f995890b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Assignment 2: Instructions\n",
    "\n",
    "In this assignment, you will take on a prediction competition for Airbnb bookings. Here, as opposed to predicting prices as we have been doing so far, you will use a variety of information provided to you to __predict the number of days a given listing will be booked in the next 30 days__. \n",
    "\n",
    "You have been provided with real listings data from Los Angeles, but you only have the actual realized bookings for a small subset of the listings, which you can use as your training data. (The column `availability_30` represents current bookings, but due to cancellations and future bookings, it's only a very noisy proxy to actual bookings). You can find the data dictionary [here](https://docs.google.com/spreadsheets/d/1iWCNJcSutYqpULSQHlNyGInUvHg2BoUGoNRIGa6Szc4/edit?gid=1322284596#gid=1322284596)\n",
    "\n",
    "This assignment will be graded as a competition. We have a test set that only the grader has access to. In order to do well in this task, you will have to use everything you have learned in the class so far, including feature engineering and hyperparameter search via cross-validation.\n",
    "\n",
    "## Write Up (8 pts)\n",
    "    \n",
    "You will need to turn in your code along with a short write-up, which you can include in your notebook. You will need to address the following components:\n",
    "\n",
    "1. (3 pts) Explain how you constructed and / or preprocessed features to help with prediction, and why.\n",
    "\n",
    "2. (3 pts) Explain what decisions you made using cross-validation, and how well you believe your final model will perform.\n",
    "\n",
    "3. (2 pts) Explain what features you found to be important using the feature importance tools we discussed in class.\n",
    "\n",
    "This write-up, along with your code, will be worth 8 points out of 15. These answers can be short: 1-3 sentences each + supporting tables or plots. \n",
    "\n",
    "## Performance (7 pts)\n",
    "The remainder of your grade will be based on your predictive accuracy, as measured in terms of $R^2$. You will recieve one point for each percentage point of test $R^2$ you achieve over 15%, rounded down, up to a maximum of 7 -- So if your test $R^2$ is 21.9% you will recieve 6 points. To recieve full credit, you will need to achieve an $R^2$ of at least 22% on the test set.\n",
    "\n",
    "In addition to this, there will be __5 points of extra credit__ available to each of the top 5 most accurate models across the whole in the class! You may use any method of your choice, even those that we have not covered, but be sure to explain it in your write-up. Also, to keep things well-scoped, __you may not pull in any datasets__ other than the one we are loading for you in the notebook (although this is a really good idea in practice!)\n",
    "\n",
    "## Submission\n",
    "You will need to submit two files: \n",
    "\n",
    "1.  Your predictions, in `.csv` format, which must have two columns: `id` and `prediction`\n",
    "2.  Your code and write up, which should be provided together as an `.ipynb` notebook.\n",
    "\n",
    "The provided notebook will get you started with loading data, and provide some checks to help you make sure your submission has the correct format. You can download `y_train.parquet` from canvas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98e3d441-485f-4d38-bf47-fbf966df27f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\prana\\anaconda3\\envs\\sklearn_env\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\prana\\anaconda3\\envs\\sklearn_env\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\prana\\anaconda3\\envs\\sklearn_env\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\prana\\anaconda3\\envs\\sklearn_env\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\prana\\anaconda3\\envs\\sklearn_env\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\prana\\anaconda3\\envs\\sklearn_env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9301d80d-57e7-48a2-8dc1-7445b438e5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\prana\\anaconda3\\envs\\sklearn_env\\lib\\site-packages (0.19.0)\n",
      "Requirement already satisfied: nltk>=3.9 in c:\\users\\prana\\anaconda3\\envs\\sklearn_env\\lib\\site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\prana\\anaconda3\\envs\\sklearn_env\\lib\\site-packages (from nltk>=3.9->textblob) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\prana\\anaconda3\\envs\\sklearn_env\\lib\\site-packages (from nltk>=3.9->textblob) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\prana\\anaconda3\\envs\\sklearn_env\\lib\\site-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\prana\\anaconda3\\envs\\sklearn_env\\lib\\site-packages (from nltk>=3.9->textblob) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\prana\\anaconda3\\envs\\sklearn_env\\lib\\site-packages (from click->nltk>=3.9->textblob) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c28bbadf-9d34-4478-af2a-4a0767bb46e9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def basic_preprocess(df):\n",
    "    df[\"price\"] = df[\"price\"].str.replace(\"$\", \"\").str.replace(\",\", \"\").astype(float)\n",
    "    df = df.dropna(subset=[\"price\"])\n",
    "    return df\n",
    "\n",
    "x_df = basic_preprocess(\n",
    "    pd.read_csv(\n",
    "        \"https://data.insideairbnb.com/united-states/ca/los-angeles/2024-09-04/data/listings.csv.gz\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Grab this from canvas and save it in this directory\n",
    "y_df = pd.read_parquet(\"y_train.parquet\")\n",
    "\n",
    "train_df = x_df.merge(y_df, on=\"id\")\n",
    "\n",
    "outer = x_df.merge(y_df, how='outer', indicator=True)\n",
    "test_df = outer[(outer._merge=='left_only')].drop('_merge', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0740ef6e-a679-4a95-8c6f-fdf4e216b429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3729, 76)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7bf4b75-bb07-453c-a006-69fca212bd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages needed\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "from geopy.distance import geodesic\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d1fe4ee-18ee-4ea4-b56e-e865cad0f841",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prana\\AppData\\Local\\Temp\\ipykernel_3056\\2862771739.py:74: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('missing', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    id                                       listing_url  \\\n",
      "0  1158417037056953812  https://www.airbnb.com/rooms/1158417037056953812   \n",
      "1             53405110             https://www.airbnb.com/rooms/53405110   \n",
      "2  1099697666994383724  https://www.airbnb.com/rooms/1099697666994383724   \n",
      "3  1039777404032438158  https://www.airbnb.com/rooms/1039777404032438158   \n",
      "4             53633927             https://www.airbnb.com/rooms/53633927   \n",
      "\n",
      "        scrape_id last_scraped       source  \\\n",
      "0  20240904164210   2024-09-05  city scrape   \n",
      "1  20240904164210   2024-09-05  city scrape   \n",
      "2  20240904164210   2024-09-05  city scrape   \n",
      "3  20240904164210   2024-09-05  city scrape   \n",
      "4  20240904164210   2024-09-05  city scrape   \n",
      "\n",
      "                                              name  \\\n",
      "0                        Lovely High Rise in Ktown   \n",
      "1                                    Downtown Loft   \n",
      "2  Chic Apartment in the Heart of Beverly Hills #8   \n",
      "3     Blueground | NoHo, walk to restaurants & art   \n",
      "4     Venice Flower Cottage: Ideal WFH + dry sauna   \n",
      "\n",
      "                                         description  \\\n",
      "0  You'll have a great time at this comfortable p...   \n",
      "1                                            missing   \n",
      "2  Welcome to our beautifully appointed apartment...   \n",
      "3  Discover the best of Los Angeles, with this th...   \n",
      "4  Welcome to the Flower Cottage Venice - your ve...   \n",
      "\n",
      "                               neighborhood_overview  \\\n",
      "0                                            missing   \n",
      "1                                            missing   \n",
      "2                                            missing   \n",
      "3  This furnished apartment is located in North H...   \n",
      "4  Venice is an absolute dreamland. From the gorg...   \n",
      "\n",
      "                                         picture_url    host_id  ...  \\\n",
      "0  https://a0.muscache.com/pictures/miso/Hosting-...  485962662  ...   \n",
      "1  https://a0.muscache.com/pictures/f0b186c4-2a75...  416339920  ...   \n",
      "2  https://a0.muscache.com/pictures/miso/Hosting-...  367097591  ...   \n",
      "3  https://a0.muscache.com/pictures/prohost-api/H...  107434423  ...   \n",
      "4  https://a0.muscache.com/pictures/miso/Hosting-...  265269171  ...   \n",
      "\n",
      "  review_scores_accuracy_bin review_scores_location_bin  \\\n",
      "0                    missing                    missing   \n",
      "1                    missing                    missing   \n",
      "2                     medium                     medium   \n",
      "3                    missing                    missing   \n",
      "4                     medium                     medium   \n",
      "\n",
      "  review_scores_value_bin review_scores_checkin_bin  \\\n",
      "0                 missing                   missing   \n",
      "1                 missing                   missing   \n",
      "2                  medium                    medium   \n",
      "3                 missing                   missing   \n",
      "4                  medium                    medium   \n",
      "\n",
      "  availability_30_to_60_ratio availability_30_to_90_ratio  \\\n",
      "0                    0.491803                    0.329670   \n",
      "1                    0.491803                    0.329670   \n",
      "2                    0.326087                    0.197368   \n",
      "3                    0.000000                    0.000000   \n",
      "4                    0.031250                    0.016949   \n",
      "\n",
      "  availability_30_to_365_ratio availability_change_30_to_60  \\\n",
      "0                     0.111111                           30   \n",
      "1                     0.082192                           30   \n",
      "2                     0.057692                           30   \n",
      "3                     0.000000                            0   \n",
      "4                     0.016949                           30   \n",
      "\n",
      "   availability_change_30_to_90 availability_change_30_to_365  \n",
      "0                            60                           239  \n",
      "1                            60                           334  \n",
      "2                            60                           244  \n",
      "3                             0                           156  \n",
      "4                            57                            57  \n",
      "\n",
      "[5 rows x 109 columns]\n",
      "     id                        listing_url       scrape_id last_scraped  \\\n",
      "0   109   https://www.airbnb.com/rooms/109  20240904164210   2024-09-05   \n",
      "1  2708  https://www.airbnb.com/rooms/2708  20240904164210   2024-09-05   \n",
      "2  2732  https://www.airbnb.com/rooms/2732  20240904164210   2024-09-05   \n",
      "3  6931  https://www.airbnb.com/rooms/6931  20240904164210   2024-09-05   \n",
      "4  7992  https://www.airbnb.com/rooms/7992  20240904164210   2024-09-05   \n",
      "\n",
      "        source                                               name  \\\n",
      "0  city scrape  Amazing bright elegant condo park front *UPGRA...   \n",
      "1  city scrape  Runyon Canyon | Beau Furn Mirror Mini-Suite Fi...   \n",
      "2  city scrape                              Zen Life at the Beach   \n",
      "3  city scrape  RUN Runyon, Beau Furn Rms w/ Stunning Terrace ...   \n",
      "4  city scrape              Quiet/Cozy/Clean/Walkable Quaint Area   \n",
      "\n",
      "                                         description  \\\n",
      "0  *** Unit upgraded with new bamboo flooring, ne...   \n",
      "1  Run Runyon Canyon, Our Gym & Sauna Open <br />...   \n",
      "2                An oasis of tranquility awaits you.   \n",
      "3  Run Runyon Canyon and Views<br /><br />Gym & S...   \n",
      "4  Hello, Traveler. Please inquire about dates be...   \n",
      "\n",
      "                               neighborhood_overview  \\\n",
      "0                                            missing   \n",
      "1  Walk and run to Runyon Canyon, it is open!<br ...   \n",
      "2  This is the best part of Santa Monica. Quiet, ...   \n",
      "3  We are in the middle of one of the great citie...   \n",
      "4  Atwater Village has a variety of great shops a...   \n",
      "\n",
      "                                         picture_url  host_id  ...  \\\n",
      "0  https://a0.muscache.com/pictures/miso/Hosting-...      521  ...   \n",
      "1  https://a0.muscache.com/pictures/miso/Hosting-...     3008  ...   \n",
      "2  https://a0.muscache.com/pictures/1082974/0f74c...     3041  ...   \n",
      "3  https://a0.muscache.com/pictures/miso/Hosting-...     3008  ...   \n",
      "4  https://a0.muscache.com/pictures/d46d140a-58db...    22363  ...   \n",
      "\n",
      "  review_scores_accuracy_bin review_scores_location_bin  \\\n",
      "0                     medium                     medium   \n",
      "1                     medium                     medium   \n",
      "2                     medium                     medium   \n",
      "3                     medium                     medium   \n",
      "4                     medium                     medium   \n",
      "\n",
      "  review_scores_value_bin review_scores_checkin_bin  \\\n",
      "0                     low                       low   \n",
      "1                  medium                    medium   \n",
      "2                  medium                    medium   \n",
      "3                  medium                    medium   \n",
      "4                  medium                    medium   \n",
      "\n",
      "  availability_30_to_60_ratio availability_30_to_90_ratio  \\\n",
      "0                    0.000000                    0.000000   \n",
      "1                    0.444444                    0.102564   \n",
      "2                    0.436364                    0.282353   \n",
      "3                    0.875000                    0.875000   \n",
      "4                    0.000000                    0.000000   \n",
      "\n",
      "  availability_30_to_365_ratio availability_change_30_to_60  \\\n",
      "0                     0.000000                            0   \n",
      "1                     0.012739                            4   \n",
      "2                     0.073394                           30   \n",
      "3                     0.026119                            0   \n",
      "4                     0.000000                            0   \n",
      "\n",
      "   availability_change_30_to_90 availability_change_30_to_365  \n",
      "0                             0                            82  \n",
      "1                            34                           309  \n",
      "2                            60                           302  \n",
      "3                             0                           260  \n",
      "4                             0                           117  \n",
      "\n",
      "[5 rows x 109 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prana\\AppData\\Local\\Temp\\ipykernel_3056\\2862771739.py:74: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('missing', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "def feature_engineering(df):\n",
    "    # 1. Sentiment Analysis on `description`\n",
    "    df['description_sentiment'] = df['description'].apply(lambda x: TextBlob(str(x)).sentiment.polarity)\n",
    "\n",
    "    # 2. Amenity Parsing: Create binary columns for popular amenities\n",
    "    popular_amenities = ['Wifi', 'Pool', 'Washer', 'Dryer', 'Air conditioning', \n",
    "                         'Parking', 'Kitchen', 'Breakfast', 'Self check-in', 'Hot tub']\n",
    "    for amenity in popular_amenities:\n",
    "        df[f'has_{amenity.lower().replace(\" \", \"_\")}'] = df['amenities'].apply(\n",
    "            lambda x: 1 if amenity in ast.literal_eval(x) else 0\n",
    "        )\n",
    "\n",
    "    # 3. Host Tenure: Calculate the number of days since the host started\n",
    "    df['host_since'] = pd.to_datetime(df['host_since'], errors='coerce')  # Handle parsing errors\n",
    "    df['host_tenure_days'] = (pd.to_datetime('today') - df['host_since']).dt.days\n",
    "\n",
    "    # 4. Proximity to Downtown LA: Compute distance of each listing to Downtown LA\n",
    "    downtown_coords = (34.052235, -118.243683)  # Downtown LA coordinates\n",
    "    df['distance_to_downtown'] = df.apply(\n",
    "        lambda row: geodesic((row['latitude'], row['longitude']), downtown_coords).km, axis=1\n",
    "    )\n",
    "\n",
    "    # 5. Recency of Reviews: Calculate the number of days since the last review\n",
    "    df['last_review'] = pd.to_datetime(df['last_review'], errors='coerce')\n",
    "    df['days_since_last_review'] = (pd.to_datetime('today') - df['last_review']).dt.days\n",
    "\n",
    "    # 6. Price per Guest: Calculate normalized price based on the number of guests accommodated\n",
    "    df['price_per_guest'] = df['price'] / df['accommodates'].replace(0, 1)\n",
    "\n",
    "    # 7. Binary Variables for Categorical Features: Convert categorical and binary columns to numerical format\n",
    "    # Create dummy variables for `room_type` and `neighbourhood_group_cleansed`\n",
    "    room_type_dummies = pd.get_dummies(df['room_type'], prefix='room_type', drop_first=True)\n",
    "    neighbourhood_dummies = pd.get_dummies(df['neighbourhood_group_cleansed'], prefix='neighbourhood', drop_first=True)\n",
    "    df = pd.concat([df, room_type_dummies, neighbourhood_dummies], axis=1)\n",
    "\n",
    "    # Convert binary columns with True/False values to 1/0\n",
    "    binary_columns = ['host_has_profile_pic', 'host_identity_verified', 'instant_bookable', 'host_is_superhost']\n",
    "    for col in binary_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].map({'t': 1, 'f': 0})\n",
    "\n",
    "    # 8. Discretization of Review Scores: Categorize review scores into bins (low, medium, high)\n",
    "    review_columns = [\n",
    "        'review_scores_rating', 'review_scores_cleanliness', 'review_scores_communication',\n",
    "        'review_scores_accuracy', 'review_scores_location', 'review_scores_value',\n",
    "        'review_scores_checkin'\n",
    "    ]\n",
    "    bins = [1, 4, 7, 10]  # Low: 1–4, Medium: 5–7, High: 8–10\n",
    "    labels = ['low', 'medium', 'high']\n",
    "    for col in review_columns:\n",
    "        df[f'{col}_bin'] = pd.cut(df[col], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "    # 9. Availability Ratios: Compute short-term to longer-term availability ratios\n",
    "    df['availability_30_to_60_ratio'] = df['availability_30'] / (df['availability_60'] + 1)\n",
    "    df['availability_30_to_90_ratio'] = df['availability_30'] / (df['availability_90'] + 1)\n",
    "    df['availability_30_to_365_ratio'] = df['availability_30'] / (df['availability_365'] + 1)\n",
    "\n",
    "    # 10. Availability Changes: Calculate differences in availability between consecutive time periods\n",
    "    df['availability_change_30_to_60'] = df['availability_60'] - df['availability_30']\n",
    "    df['availability_change_30_to_90'] = df['availability_90'] - df['availability_30']\n",
    "    df['availability_change_30_to_365'] = df['availability_365'] - df['availability_30']\n",
    "\n",
    "\n",
    "    # 11. Handle Missing Values\n",
    "    # Fill numeric missing values with the median\n",
    "    df.fillna(df.median(numeric_only=True), inplace=True)\n",
    "\n",
    "    # For categorical columns, add 'missing' as a category and fill missing values\n",
    "    categorical_columns = df.select_dtypes(include=['category', 'object']).columns\n",
    "    for col in categorical_columns:\n",
    "        if df[col].dtype.name == 'category':\n",
    "            # Add 'missing' as a category\n",
    "            df[col] = df[col].cat.add_categories('missing')\n",
    "        df[col].fillna('missing', inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply Feature Engineering to Train and Test Datasets\n",
    "train_df = feature_engineering(train_df)\n",
    "test_df = feature_engineering(test_df)\n",
    "\n",
    "# Print the first few rows to verify transformations\n",
    "print(train_df.head())\n",
    "print(test_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66038665-2933-41c0-9d51-f1e1de03e244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Training Dataset\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# 4. Select variables to be used in training\n",
    "selected_columns = [\n",
    "    'availability_change_30_to_60', 'availability_change_30_to_90', 'availability_change_30_to_365', 'description_sentiment',\n",
    "    'has_wifi', 'has_pool', 'has_dryer', 'has_washer', 'has_hot_tub', 'has_kitchen', 'has_air_conditioning',\n",
    "    'has_parking', 'host_tenure_days', 'price_per_guest', 'distance_to_downtown',\n",
    "    'review_scores_rating_bin', 'review_scores_accuracy_bin', 'review_scores_cleanliness_bin',\n",
    "    'review_scores_checkin_bin', 'review_scores_communication_bin', 'review_scores_location_bin',\n",
    "    'review_scores_value_bin', 'days_since_last_review', 'minimum_nights', 'reviews_per_month',\n",
    "    'number_of_reviews_ltm', 'number_of_reviews_l30d', 'host_acceptance_rate', 'host_response_rate',\n",
    "    'room_type', 'neighbourhood_group_cleansed', 'host_has_profile_pic', 'host_identity_verified',\n",
    "    'instant_bookable', 'host_is_superhost'\n",
    "]\n",
    "\n",
    "# 5. Filter out columns not present in the training dataset\n",
    "selected_columns = [col for col in selected_columns if col in train_df.columns]\n",
    "\n",
    "# 6. Define features (X) and target variable (y)\n",
    "X = train_df[selected_columns]\n",
    "y = train_df['days_booked']\n",
    "\n",
    "# 7. Separate features into categorical and numerical columns\n",
    "categorical_columns = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "numerical_columns = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# 8. Create a preprocessing pipeline\n",
    "# - Apply one-hot encoding to categorical columns\n",
    "# - Impute missing values and scale numerical columns\n",
    "transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), categorical_columns),\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value=0)),  # Impute missing values with 0\n",
    "            ('scaler', StandardScaler())  # Standardize numerical features\n",
    "        ]), numerical_columns)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 9. Fit the transformer to the data and apply the transformations\n",
    "transformer.fit(X)\n",
    "X_transformed = transformer.transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9084a9c6-cbd8-4b84-9b01-7a960275007c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "Best Cross-Validation R²: 0.2047\n"
     ]
    }
   ],
   "source": [
    "# use random forest and grid search to find the best model\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the Random Forest model\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Define the parameter grid for Random Forest\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [50, 100],  # Reduce number of trees\n",
    "    'max_depth': [10, 20],      # Fewer depth options\n",
    "    'min_samples_split': [2, 5], \n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "  # Minimum samples in a leaf node\n",
    "\n",
    "# Set up GridSearchCV for Random Forest\n",
    "rf_grid_search = GridSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_grid=rf_param_grid,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring='r2',  # Use R² as the scoring metric\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the GridSearchCV on the preprocessed data\n",
    "rf_grid_search.fit(X_transformed, y)\n",
    "\n",
    "# Get the best model and parameters\n",
    "best_rf_model = rf_grid_search.best_estimator_\n",
    "print(f\"Best Parameters: {rf_grid_search.best_params_}\")\n",
    "print(f\"Best Cross-Validation R²: {rf_grid_search.best_score_:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64061392-893d-46f1-903b-e21eafa3733d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "Best Parameters (XGBoost): {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50, 'subsample': 1.0}\n",
      "Best Cross-Validation R² (XGBoost): 0.2188\n"
     ]
    }
   ],
   "source": [
    "# Use XGBoost for regression and tune hyperparameters\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the XGBoost model\n",
    "xgb_model = XGBRegressor(random_state=42)\n",
    "\n",
    "# Define the parameter grid for XGBoost\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV for XGBoost\n",
    "xgb_grid_search = GridSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_grid=xgb_param_grid,\n",
    "    cv=5,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    "    verbose=3\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV on the training data\n",
    "xgb_grid_search.fit(X_transformed, y)\n",
    "\n",
    "# Get the best model and parameters\n",
    "best_xgb_model = xgb_grid_search.best_estimator_\n",
    "print(f\"Best Parameters (XGBoost): {xgb_grid_search.best_params_}\")\n",
    "print(f\"Best Cross-Validation R² (XGBoost): {xgb_grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbb49dec-2fff-48ca-b7f0-d9dae427cb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Regressor Training R²: 0.4672\n"
     ]
    }
   ],
   "source": [
    "# Use Ensemble Learning to Combine Predictions from Multiple Models\n",
    "\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "# Initialize the Voting Regressor with the best models\n",
    "# 'rf' for Random Forest and 'xgb' for XGBoost\n",
    "voting_model = VotingRegressor(estimators=[\n",
    "    ('rf', best_rf_model),  # Pre-trained Random Forest model\n",
    "    ('xgb', best_xgb_model)  # Pre-trained XGBoost model\n",
    "])\n",
    "\n",
    "# Train the Voting Regressor on the full transformed training dataset\n",
    "voting_model.fit(X_transformed, y)\n",
    "\n",
    "# Generate predictions using the trained Voting Regressor\n",
    "y_train_pred = voting_model.predict(X_transformed)\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Calculate and display the R² score for the Voting Regressor on the training data\n",
    "train_r2 = r2_score(y, y_train_pred)\n",
    "print(f\"Voting Regressor Training R²: {train_r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "818d5fb4-612a-449d-8a28-3ce95035ecc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation R² Scores: [0.2421598  0.27912535 0.20894832 0.18895483 0.18081076]\n",
      "Mean Cross-Validation R²: 0.2200\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Evaluate the Voting Regressor using cross-validation\n",
    "cv_scores = cross_val_score(voting_model, X_transformed, y, cv=5, scoring='r2')\n",
    "\n",
    "# Display cross-validation scores\n",
    "print(\"Cross-Validation R² Scores:\", cv_scores)\n",
    "print(f\"Mean Cross-Validation R²: {cv_scores.mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db47deb-d80b-404f-aa7b-32e5454a6d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the final Voting Regressor on the entire training dataset\n",
    "voting_model.fit(X_transformed, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817457cf-68ac-4021-bcfb-d06c1f89b3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the test dataset using the same transformer\n",
    "X_test_transformed = transformer.transform(test_df[selected_columns])\n",
    "\n",
    "# Generate predictions for the test set\n",
    "y_test_pred = voting_model.predict(X_test_transformed)\n",
    "\n",
    "# Preview the first few predictions\n",
    "print(\"Predictions for the Test Set:\", y_test_pred[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb07df1-633c-43a5-8b23-d6140adb2bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Select the same features from test_df as used in training\n",
    "selected_columns = [\n",
    "    'availability_change_30_to_60', 'availability_change_30_to_90', 'availability_change_30_to_365',\n",
    "    'description_sentiment', 'has_wifi', 'has_pool', 'has_dryer', 'has_washer', 'has_hot_tub',\n",
    "    'has_kitchen', 'has_air_conditioning', 'has_parking', 'host_tenure_days', 'price_per_guest',\n",
    "    'distance_to_downtown', 'review_scores_rating_bin', 'review_scores_accuracy_bin',\n",
    "    'review_scores_cleanliness_bin', 'review_scores_checkin_bin', 'review_scores_communication_bin',\n",
    "    'review_scores_location_bin', 'review_scores_value_bin', 'days_since_last_review',\n",
    "    'minimum_nights', 'reviews_per_month', 'number_of_reviews_ltm', 'number_of_reviews_l30d',\n",
    "    'host_acceptance_rate', 'host_response_rate', 'room_type', 'neighbourhood_group_cleansed',\n",
    "    'host_has_profile_pic', 'host_identity_verified', 'instant_bookable', 'host_is_superhost'\n",
    "]\n",
    "\n",
    "# Ensure only columns that exist in test_df are used\n",
    "selected_columns = [col for col in selected_columns if col in test_df.columns]\n",
    "\n",
    "# Preprocess the test data\n",
    "X_test = test_df[selected_columns]  # Select relevant features\n",
    "X_test_transformed = transformer.transform(X_test)  # Apply the transformer\n",
    "\n",
    "# Make predictions on the test set using Voting Regressor\n",
    "y_test_pred = voting_model.predict(X_test_transformed)\n",
    "\n",
    "# Create a DataFrame with only 'id' and 'prediction'\n",
    "output_df = test_df[['id']].copy()  # Ensure only the 'id' column is selected\n",
    "output_df['prediction'] = y_test_pred  # Add the predictions as a new column\n",
    "\n",
    "# Save the output DataFrame to CSV\n",
    "output_df.to_csv('predictions.csv', index=False)\n",
    "\n",
    "print(\"Predictions saved to 'predictions.csv' with only 'id' and 'prediction' columns.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0538d91-98c8-41d5-b767-f183b4023e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to make sure that test_df has correct format and write to csv\n",
    "from hashlib import md5\n",
    "import numpy as np\n",
    "# Checks that you have predictions for the expected listing ids \n",
    "assert md5(np.sort(test_df.id.values)).hexdigest() == '87ed95adc911aad0ed9ef119a7a3315d', \"Your listing ids are incorrect; you may need to regenerate test_df in the first cell\"\n",
    "assert \"prediction\" in test_df.columns, \"You need to have a column named `prediction` in your output\"\n",
    "# Submit this CSV on canvas\n",
    "test_df.to_csv(\"predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdca4b7-8d7c-49e0-ace9-0191691a978e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65682d35-2516-4cb4-94d1-772834e83bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Random Forest feature importance\n",
    "rf_feature_importance = pd.Series(\n",
    "    best_rf_model.feature_importances_,\n",
    "    index=transformer.get_feature_names_out()\n",
    ")\n",
    "rf_feature_importance_sorted = rf_feature_importance.sort_values(ascending=False)\n",
    "\n",
    "# Plot the top 10 features\n",
    "plt.figure(figsize=(12, 8))  # Adjust figure size for better readability\n",
    "top_features = rf_feature_importance_sorted.head(10)\n",
    "\n",
    "ax = top_features.plot(\n",
    "    kind='bar',\n",
    "    title='Top 10 Random Forest Feature Importances',\n",
    "    color='orange',  # Changed bar color\n",
    "    edgecolor='black'  # Add edge color for better contrast\n",
    ")\n",
    "\n",
    "# Add labels and grid\n",
    "plt.ylabel('Feature Importance Score', fontsize=12)\n",
    "plt.xlabel('Features', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=10)  # Rotate feature names for better visibility\n",
    "plt.yticks(fontsize=10)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add feature importance values to the bars\n",
    "for i, value in enumerate(top_features):\n",
    "    ax.text(i, value + 0.005, f'{value:.3f}', ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to prevent clipping\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c6ef00-cd72-4797-8a14-86aa2765f55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# XGBoost feature importance\n",
    "xgb_feature_importance = pd.Series(\n",
    "    best_xgb_model.feature_importances_,\n",
    "    index=transformer.get_feature_names_out()\n",
    ")\n",
    "xgb_feature_importance_sorted = xgb_feature_importance.sort_values(ascending=False)\n",
    "\n",
    "# Plot the top 10 features\n",
    "plt.figure(figsize=(12, 8))  # Adjust figure size for better readability\n",
    "top_features = xgb_feature_importance_sorted.head(10)\n",
    "\n",
    "ax = top_features.plot(\n",
    "    kind='bar',\n",
    "    title='Top 10 XGBoost Feature Importances',\n",
    "    color='teal',  # Changed bar color\n",
    "    edgecolor='black'  # Add edge color for better contrast\n",
    ")\n",
    "\n",
    "# Add labels and grid\n",
    "plt.ylabel('Feature Importance Score', fontsize=12)\n",
    "plt.xlabel('Features', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=10)  # Rotate feature names for better visibility\n",
    "plt.yticks(fontsize=10)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add feature importance values to the bars\n",
    "for i, value in enumerate(top_features):\n",
    "    ax.text(i, value + 0.005, f'{value:.3f}', ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to prevent clipping\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e93f289-8ced-4aff-99ea-4d3d76173bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Combine feature importances by averaging\n",
    "combined_feature_importance = (rf_feature_importance + xgb_feature_importance) / 2\n",
    "\n",
    "# Sort and visualize the top 10 combined features\n",
    "combined_feature_importance_sorted = combined_feature_importance.sort_values(ascending=False)\n",
    "top_combined_features = combined_feature_importance_sorted.head(10)\n",
    "\n",
    "# Plot the top 10 combined feature importances\n",
    "plt.figure(figsize=(12, 8))  # Adjust figure size for better readability\n",
    "\n",
    "ax = top_combined_features.plot(\n",
    "    kind='bar',\n",
    "    title='Top 10 Combined Feature Importances (Random Forest & XGBoost)',\n",
    "    color='purple',  # Changed bar color\n",
    "    edgecolor='black'  # Add edge color for better contrast\n",
    ")\n",
    "\n",
    "# Add labels and grid\n",
    "plt.ylabel('Average Feature Importance', fontsize=12)\n",
    "plt.xlabel('Features', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=10)  # Rotate feature names for better visibility\n",
    "plt.yticks(fontsize=10)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add feature importance values to the bars\n",
    "for i, value in enumerate(top_combined_features):\n",
    "    ax.text(i, value + 0.005, f'{value:.3f}', ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to prevent clipping\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14efe680-efa1-4f33-a63f-5d8e412abc85",
   "metadata": {},
   "source": [
    "### Write-Up (8 pts)\r\n",
    "\r\n",
    "#### 1. Explain how you constructed and/or preprocessed features to help with prediction, and why. (3 pts)\r\n",
    "\r\n",
    "To prepare the data for modeling, I implemented extensive feature engineering and preprocessing steps to maximize the predictive power of the model:\r\n",
    "\r\n",
    "1. **Feature Engineering:**\r\n",
    "   - **Sentiment Analysis:** Extracted sentiment polarity from the `description` field to capture nuances in listing descriptions.\r\n",
    "   - **Binary Variables for Popular Amenities:** Created binary indicators for key amenities like WiFi, Pool, Kitchen, and Parking to quantify their availability in listings.\r\n",
    "   - **Host Tenure and Activity:** Computed the number of days since the host joined (`host_tenure_days`) and days since the last review (`days_since_last_review`) to reflect host reliability and recent activity.\r\n",
    "   - **Proximity to Downtown LA:** Calculated the geodesic distance between the listing and Downtown LA to account for location desirability.\r\n",
    "   - **Normalized Pricing Metrics:** Derived `price_per_guest` to adjust prices based on guest capacity for better comparability across listings.\r\n",
    "   - **Discretized Review Scores:** Categorized review scores into bins (low, medium, high) for interpretability and to reduce noise in continuous values.\r\n",
    "   - **Availability Ratios and Changes:** Created ratios (e.g., `availability_30_to_60_ratio`) and differences (e.g., `availability_change_30_to_60`) to capture temporal availability trends.\r\n",
    "\r\n",
    "2. **Preprocessing Steps:**\r\n",
    "   - **Categorical Encoding:** Applied one-hot encoding to categorical variables like `room_type` and `neighbourhood_group_cleansed`.\r\n",
    "   - **Scaling and Imputation:** Used standard scaling for numerical features and imputed missing values with the median or default values, ensuring consistent handling of missing data.\r\n",
    "\r\n",
    "These steps were designed to capture meaningful patterns in the data and ensure compatibility with machine learning models. Together, they provided a comprehensive and clean feature set for training.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "#### 2. Explain what decisions you made using cross-validation, and how well you believe your final model will perform. (3 pts)\r\n",
    "\r\n",
    "To ensure that the model generalizes well to unseen data, I made several decisions based on cross-validation:\r\n",
    "\r\n",
    "1. **Cross-Validation Strategy:**\r\n",
    "   - I employed **5-fold cross-validation** to evaluate model performance. This method splits the training data into five subsets, using four subsets for training and one for validation in each iteration. This process helps ensure that the model performs consistently across different data splits, reducing the risk of overfitting or underfitting.\r\n",
    "   - The mean \\(R^2\\) score from cross-validation provides an unbiased estimate of the model's ability to generalize.\r\n",
    "\r\n",
    "2. **Model Selection and Tuning:**\r\n",
    "   - Based on cross-validation scores:\r\n",
    "     - I tested several models, including **Lasso Regression**, **Random Forest**, and **XGBoost**, to identify the best-performing approach.\r\n",
    "     - For Lasso, despite hyperparameter tuning, the \\(R^2\\) remained at 15-17%, which was not sufficient. Therefore, I excluded it from the final notebook.\r\n",
    "     - I selected a **Voting Regressor** combining **Random Forest** and **XGBoost**, as it aggregates the strengths of both models:\r\n",
    "       - Random Forest captures interactions in smaller subsets of data.\r\n",
    "       - XGBoost learns nuanced patterns through gradient boosting.\r\n",
    "\r\n",
    "3. **Evaluation Metrics:**\r\n",
    "   - The **mean cross-validation \\(R^2\\)** score for the Voting Regressor was **0.2200**, which meets the assignment’s threshold of 0.22 for test set performance. This suggests that the model has learned patterns in the data that are likely to generalize well to unseen examples.\r\n",
    "\r\n",
    "4. **Confidence in Test Set Performance:**\r\n",
    "   - While the **training \\(R^2\\)** was relatively high (0.4672), cross-validation results indicate that the model avoids significant overfitting.\r\n",
    "   - The inherent noise in the dataset (e.g., cancellations, incomplete bookings, and missing data) makes achieving high \\(R^2\\) scores challenging. However, the cross-validation \\(R^2\\) of 0.2200 reflects robust generalization.\r\n",
    "\r\n",
    "In conclusion, cross-validation and careful hyperparameter tuning were critical in selecting and optimizing the Voting Regressor. Its strong performance on validation data gives confidence in its ability to generalize to the test set.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "#### 3. Explain what features you found to be important using the feature importance tools we discussed in class. (2 pts)\r\n",
    "\r\n",
    "Feature importance analysis using **Random Forest**, **XGBoost**, and the combined average highlighted the following key predictors:\r\n",
    "\r\n",
    "1. **Top Features Identified:**\r\n",
    "   - `num__number_of_reviews_ltm`: The number of reviews in the last 12 months was consistently the most important feature across both models. It reflects listing popularity and recent activity.\r\n",
    "   - `num__availability_change_30_to_90`: Changes in availability over 90 days provide insights into booking patterns and future demand.\r\n",
    "   - `num__distance_to_downtown`: Listings closer to Downtown LA are generally more desirable, impacting booking likelihood.\r\n",
    "   - `num__price_per_guest`: Normalized pricing helps capture the affordability and value of listings.\r\n",
    "   - `num__days_since_last_review`: Recent reviews indicate active bookings, which influence predictions.\r\n",
    "\r\n",
    "2. **Insights from Combined Importance:**\r\n",
    "   - Aggregating feature importances from Random Forest and XGBoost showed that both models heavily relied on temporal availability changes and review-related metrics, confirming the relevance of these features.\r\n",
    "\r\n",
    "3. **Why Feature Importance?**\r\n",
    "   - Feature importance was chosen over permutation importance due to its computational efficiency, especially with large datasets or complex models. While permutation importance provides more robust insights by directly assessing feature impact on predictions, feature importance effectively captures the relative contributions of features and is more practical for this assignment.\r\n",
    "\r\n",
    "4. **Visualizations:**\r\n",
    "   - The top 10 features for each model were visualized using bar plots. These plots provide a clear picture of the most impactful predictors for bookings.\r\n",
    "\r\n",
    "In summary, feature importance analysis verified the relevance of features derived from availability, pricing, location, and reviews. These insights align with domain knowledge and emphasize the robustness of the engineered features and the interpretability and reliability of the model.\r\n",
    " features.\r\n",
    "the interpretability and reliability of the model.\r\n",
    "\r\n",
    "---\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7addff9-b691-4a42-bfdc-34849e94012d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e490dc-10ad-42f4-9f77-697118e1d892",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sklearn_env)",
   "language": "python",
   "name": "sklearn_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "name": "Untitled.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
